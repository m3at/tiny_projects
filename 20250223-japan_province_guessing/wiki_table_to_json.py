#!/usr/bin/env python3

"""
[file mainly generated by LLMs]

Uses the data from the following wikipedia pages:

https://en.wikipedia.org/wiki/Prefectures_of_Japan
https://en.wikipedia.org/wiki/List_of_Japanese_prefectures_by_highest_mountain
https://ja.wikipedia.org/wiki/%E5%90%84%E9%83%BD%E9%81%93%E5%BA%9C%E7%9C%8C%E3%81%AE%E6%9C%80%E9%AB%98%E5%B3%B0
"""


import argparse
import json
import logging
import re
import sys
import unicodedata
from pathlib import Path

logger = logging.getLogger("parse_wiki_table")


def normalize_prefecture_name(name: str) -> str:
    """
    Normalize a prefecture name by removing diacritical marks.
    E.g. "Tōkyō" becomes "Tokyo", "Ōsaka" becomes "Osaka"
    """
    nfkd_form = unicodedata.normalize("NFKD", name)
    return "".join([c for c in nfkd_form if not unicodedata.combining(c)]).strip()


def clean_cell(cell: str) -> str:
    """
    Remove leading/trailing whitespace and strip out cell formatting.
    This function now removes a leading "|" if present,
    and then removes any formatting like: align="right"| or bgcolor=...|
    """
    cell = cell.strip()
    if cell.startswith("|"):
        cell = cell[1:].strip()
    cell = re.sub(r'^(?:align="[^"]*"|bgcolor=[^|]+)\|\s*', "", cell)
    return cell.strip()


def parse_flag(cell: str) -> str:
    """
    Extract the English name from a flag macro.
    E.g. "{{flag|Aichi}}" or "{{flag|Chiba|Chiba Prefecture}}" returns "Aichi".
    """
    m = re.search(r"{{flag\|([^}|]+)", cell)
    if m:
        return m.group(1).strip()
    return cell


def parse_nihongo(cell: str) -> str:
    """
    Extract the Japanese name from a nihongo2 macro.
    E.g. "{{nihongo2|岩手県}}" returns "岩手県".
    """
    m = re.search(r"{{nihongo2\|([^}]+)}}", cell)
    if m:
        return m.group(1).strip()
    return cell


def parse_wikilink(cell: str) -> str:
    """
    Extract the display text from a wikilink.
    E.g. "[[Morioka]]" returns "Morioka" and
         "[[Akita City|Akita]]" returns "Akita".
    """
    m = re.search(r"\[\[(?:[^\]|]+\|)?([^\]]+)\]\]", cell)
    if m:
        return m.group(1).strip()
    return cell


def parse_change(cell: str) -> int:
    """
    Extract the first number from the change macro which is the estimated population.
    E.g. "{{change|invert=on|1168771|1210534}}" returns 1168771.
    """
    m = re.search(r"{{change\|[^|]*\|(\d+)", cell)
    if m:
        return int(m.group(1))
    try:
        return int(cell.replace(",", ""))
    except ValueError:
        return 0


def parse_iso(cell: str) -> str:
    """
    Extract the numeric part from the ISO code.
    E.g. "JP-03" returns "3" (leading zeros are dropped).
    """
    m = re.search(r"JP-0*(\d+)", cell)
    if m:
        return str(int(m.group(1)))
    return cell


def extract_summit_data_en(summit_file: Path) -> dict:
    """
    Process the English summit table.
    Expected columns:
      0: Prefecture (wikilink) – used as key.
      1: Mountain (wikilink) → highest_peak_name_en.
      2: Elevation (number, may contain commas) → highest_peak_elevation.
      3: Cordillera (wikilink) → highest_peak_cordillera_en.
    We normalize the prefecture name to ease matching.
    """
    logger.info(f"Reading English summit data from {summit_file}")
    with summit_file.open("r", encoding="utf-8") as f:
        lines = f.read().splitlines()

    rows = []
    current_row = []
    for line in lines:
        line = line.strip()
        if line.startswith("|-"):
            if current_row:
                rows.append(current_row)
            current_row = []
        else:
            if line.startswith("|") or line.startswith("!"):
                current_row.append(line)
    if current_row:
        rows.append(current_row)

    logger.info(f"Found {len(rows)} rows in the English summit table")
    summit_data = {}
    for row in rows:
        if any(cell.startswith("!") for cell in row):
            continue
        if len(row) < 5:
            logger.debug("Skipping row with insufficient cells in English summit table")
            continue
        try:
            prefecture_cell = clean_cell(row[0])
            mountain_cell = clean_cell(row[1])
            elevation_cell = clean_cell(row[2])
            cordillera_cell = clean_cell(row[3])

            prefecture_name = parse_wikilink(prefecture_cell)
            # Normalize the prefecture name to match the main table
            norm_prefecture_name = normalize_prefecture_name(prefecture_name)

            peak_name_en = parse_wikilink(mountain_cell)
            elevation = int(elevation_cell.replace(",", ""))
            cordillera_en = parse_wikilink(cordillera_cell)

            summit_data[norm_prefecture_name] = {
                "highest_peak_name_en": peak_name_en,
                "highest_peak_elevation": elevation,
                "highest_peak_cordillera_en": cordillera_en,
            }
        except Exception as e:
            logger.error(f"Error processing English summit row: {row}. Error: {e}")
    return summit_data


def extract_summit_data_ja(summit_file: Path) -> dict:
    """
    Process the Japanese summit table.
    Expected columns:
      0: 番号 (prefecture ID, matching the numeric part from ISO)
      1: 都道府県 (prefecture name in Japanese – not used for matching)
      2: 山名 (mountain name) → highest_peak_name_ja.
      3: 標高 (elevation) → highest_peak_elevation.
      4: 山系 (cordillera) → highest_peak_cordillera_ja.
    """
    logger.info(f"Reading Japanese summit data from {summit_file}")
    with summit_file.open("r", encoding="utf-8") as f:
        lines = f.read().splitlines()

    rows = []
    current_row = []
    for line in lines:
        line = line.strip()
        if line.startswith("|-"):
            if current_row:
                rows.append(current_row)
            current_row = []
        else:
            if line.startswith("|") or line.startswith("!"):
                current_row.append(line)
    if current_row:
        rows.append(current_row)

    logger.info(f"Found {len(rows)} rows in the Japanese summit table")
    summit_data = {}
    for row in rows:
        if any(cell.startswith("!") for cell in row):
            continue
        if len(row) < 6:
            logger.debug(
                "Skipping row with insufficient cells in Japanese summit table"
            )
            continue
        try:
            id_cell = clean_cell(row[0])
            mountain_cell = clean_cell(row[2])
            elevation_cell = clean_cell(row[3])
            cordillera_cell = clean_cell(row[4])

            prefecture_id = str(int(id_cell))
            peak_name_ja = parse_wikilink(mountain_cell)
            elevation = int(elevation_cell.replace(",", ""))
            cordillera_ja = parse_wikilink(cordillera_cell)

            summit_data[prefecture_id] = {
                "highest_peak_name_ja": peak_name_ja,
                "highest_peak_elevation": elevation,
                "highest_peak_cordillera_ja": cordillera_ja,
            }
        except Exception as e:
            logger.error(f"Error processing Japanese summit row: {row}. Error: {e}")
    return summit_data


def extract_prefecture_data(
    raw_text_file: Path,
    output_json_file: Path,
    summit_en_file: Path,
    summit_ja_file: Path,
):
    logger.info(f"Reading raw data from {raw_text_file}")
    with raw_text_file.open("r", encoding="utf-8") as f:
        lines = f.read().splitlines()

    rows = []
    current_row = []
    for line in lines:
        line = line.strip()
        if line.startswith("|-"):
            if current_row:
                rows.append(current_row)
            current_row = []
        else:
            if line.startswith("|") or line.startswith("!"):
                current_row.append(line)
    if current_row:
        rows.append(current_row)

    logger.info(f"Found {len(rows)} rows in the prefecture table")
    data = []
    for row in rows:
        if any(cell.startswith("!") for cell in row):
            continue
        if len(row) < 13:
            logger.debug("Skipping row with insufficient cells")
            continue
        try:
            flag_cell = clean_cell(row[0])
            name_ja_cell = clean_cell(row[1])
            capital_en_cell = clean_cell(row[2])
            capital_ja_cell = clean_cell(row[3])
            population_cell = clean_cell(row[6])
            iso_cell = clean_cell(row[11])

            prefecture = {
                "name_en": parse_flag(flag_cell),
                "name_ja": parse_nihongo(name_ja_cell),
                "capital_en": parse_wikilink(capital_en_cell),
                "capital_ja": parse_nihongo(capital_ja_cell),
                "population": parse_change(population_cell),
                "id": parse_iso(iso_cell),
            }
            data.append(prefecture)
        except Exception as e:
            logger.error(f"Error processing row: {row}. Error: {e}")

    logger.info(f"Extracted data for {len(data)} prefectures")

    # Extract summit data from the two new files.
    summit_data_en = extract_summit_data_en(summit_en_file)
    summit_data_ja = extract_summit_data_ja(summit_ja_file)

    # Merge summit data into prefecture data.
    for pref in data:
        name_en = pref.get("name_en")
        pref_id = pref.get("id")
        norm_name = normalize_prefecture_name(name_en)
        if norm_name in summit_data_en:
            pref.update(summit_data_en[norm_name])
        else:
            logger.debug(f"No English summit data found for prefecture {name_en}")

        if pref_id in summit_data_ja:
            pref.update(summit_data_ja[pref_id])
        else:
            logger.debug(f"No Japanese summit data found for prefecture id {pref_id}")

    data.sort(key=lambda x: int(x["id"]))
    with output_json_file.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    logger.info(f"Data written to {output_json_file}")


def main(
    raw_file: Path, json_file: Path, summit_en_file: Path, summit_ja_file: Path
) -> None:
    extract_prefecture_data(raw_file, json_file, summit_en_file, summit_ja_file)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Extract prefecture data from raw Wikipedia table and add summit information",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument("raw_file", type=Path, help="Path to wiki_table_prefecture.txt")
    parser.add_argument("json_file", type=Path, help="Path to output JSON file")
    parser.add_argument(
        "--summit_en", type=Path, required=True, help="Path to wiki_table_summits.txt"
    )
    parser.add_argument(
        "--summit_ja",
        type=Path,
        required=True,
        help="Path to wiki_table_summits_ja.txt",
    )
    parser.add_argument(
        "--log_level",
        type=str,
        default="DEBUG",
        choices=["debug", "info", "warning", "error"],
        help="Logging level",
    )
    args = vars(parser.parse_args())
    log_level = getattr(logging, args.pop("log_level").upper())
    logger.setLevel(log_level)
    ch = logging.StreamHandler()
    ch.setLevel(log_level)
    ch.setFormatter(
        logging.Formatter(
            "{asctime} {levelname} {name:>16}│ {message}", datefmt="%H:%M:%S", style="{"
        )
    )
    logger.addHandler(ch)

    if sys.stdout.isatty():
        _m = logging.getLevelNamesMapping()
        for c, lvl in [[226, "DEBUG"], [148, "INFO"], [208, "WARNING"], [197, "ERROR"]]:
            logging.addLevelName(_m[lvl], f"\x1b[38;5;{c}m{lvl:<7}\x1b[0m")

    raw_file = args.pop("raw_file")
    json_file = args.pop("json_file")
    summit_en_file = args.pop("summit_en")
    summit_ja_file = args.pop("summit_ja")
    main(raw_file, json_file, summit_en_file, summit_ja_file)
