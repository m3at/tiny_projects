{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054274ff-50e7-489d-aecb-99adedd9b58b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import LCMScheduler, AutoPipelineForText2Image\n",
    "\n",
    "# https://huggingface.co/latent-consistency/lcm-lora-sdv1-5\n",
    "model_id = \"Lykon/dreamshaper-8\"\n",
    "adapter_id = \"latent-consistency/lcm-lora-sdv1-5\"\n",
    "\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.to(\"mps\")\n",
    "\n",
    "# nope, not for mps\n",
    "# pipe = torch.compile(pipe)\n",
    "\n",
    "# load and fuse lcm lora\n",
    "pipe.load_lora_weights(adapter_id)\n",
    "pipe.fuse_lora()\n",
    "\n",
    "# 512x512 image\n",
    "prompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # disable guidance_scale by passing 0\n",
    "    image = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0).images[0]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "742a46b7-074a-4a5b-8db8-741895d75192",
   "metadata": {},
   "source": [
    "from diffusers import AutoPipelineForText2Image, DPMSolverMultistepScheduler\n",
    "import torch\n",
    "\n",
    "pipe = AutoPipelineForText2Image.from_pretrained('lykon/dreamshaper-xl-v2-turbo', torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe = pipe.to(\"mps\")\n",
    "\n",
    "# prompt = \"portrait photo of muscular bearded guy in a worn mech suit, light bokeh, intricate, steel metal, elegant, sharp focus, soft lighting, vibrant colors\"\n",
    "prompt = \"high quality picture, award winning landscape photography of Tokyo, Japan, 4k\"\n",
    "\n",
    "generator = torch.manual_seed(0)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    image_xl = pipe(prompt, num_inference_steps=6, guidance_scale=2).images[0]  \n",
    "    # image.save(\"./image.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "52a01753-fd9f-4859-90d9-31919f86740e",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"high quality picture, award winning landscape photography of Tokyo, Japan, detailed, 8k\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    image_xl = pipe(\n",
    "        prompt, num_inference_steps=6, guidance_scale=2,\n",
    "        # width=1024, height=1024, # 60s\n",
    "        width=256, height=256, # 4s\n",
    "    ).images[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d403dc-14c0-41b6-b74a-2a8f22f24cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# mps: 3.0s\n",
    "\n",
    "# prompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n",
    "# prompt = \"high quality picture, award winning landscape photography of Tokyo, Japan, 4k\"\n",
    "# prompt = \"award winning photography of Tokyo, Japan, detailed, 8k, daytime\"\n",
    "# prompt = \"high quality picture, award winning photography of Tokyo, Japan, detailed, 8k, daytime, aesthetic, magazine cover, 8k\"\n",
    "# prompt = \"photography of Tokyo, Japan\"\n",
    "prompt = \"photography of Jersey Island, United Kingdom\"\n",
    "negative_prompt = None\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # disable guidance_scale by passing 0\n",
    "    # image = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0).images[0]\n",
    "    image = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=6,\n",
    "        # guidance_scale should be 0, or in 1-2\n",
    "        guidance_scale=2,\n",
    "        width=512, height=512, # 3s\n",
    "        # width=256, height=256, # 0.9s\n",
    "    ).images[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd6e2c-b383-4a2b-b05d-2e532b97390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f7bfa-200c-430b-8e51-3ef75f62eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Work fast, low diversity\n",
    "# https://github.com/IDKiro/sdxs\n",
    "# https://huggingface.co/IDKiro/sdxs-512-0.9\n",
    "# seed = 42\n",
    "# weight_type = torch.float32\n",
    "weight_type = torch.float16\n",
    "\n",
    "# https://github.com/IDKiro/sdxs\n",
    "# Load model.\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    # \"IDKiro/sdxs-512-0.9\",\n",
    "    \"IDKiro/sdxs-512-dreamshaper\",\n",
    "    torch_dtype=weight_type,\n",
    ")\n",
    "\n",
    "# use original VAE\n",
    "# pipe.vae = AutoencoderKL.from_pretrained(\"IDKiro/sdxs-512-0.9/vae_large\")\n",
    "pipe.to(\"mps\")\n",
    "\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "# 512x512 image\n",
    "prompt = \"high quality picture, award winning photography of Saint Pierre and Miquelon, France, detailed, daytime, aesthetic, magazine cover, 8k\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    image = pipe(\n",
    "        prompt=prompt, num_inference_steps=1, guidance_scale=0\n",
    "    ).images[0]\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c51b67f3-9eab-4671-b015-cf47179d3bf8",
   "metadata": {},
   "source": [
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "    # return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    return unicodedata.normalize('NFKD', s).encode('ASCII', 'ignore').decode()\n",
    "\n",
    "strip_accents(\"adélie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf576cf-86d6-41ba-a870-4128744722f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "place = \"Jersey Island, United Kingdom\"\n",
    "place = \"Tokyo, Japan\"\n",
    "place = \"Kerguelen Islands, France\"\n",
    "place = \"Terrer Adélie, Antarctica\"\n",
    "place = \"Clipperton Island, France\"\n",
    "place = \"Saint Pierre and Miquelon, France\"\n",
    "place = \"Saint Barthélemy, France\"\n",
    "place = \"Rome, Italy\"\n",
    "\n",
    "prompt = f\"high quality picture, award winning photography of {place}, detailed, daytime, aesthetic, 8k\"\n",
    "# prompt = f\"award winning photography of {place}\"\n",
    "\n",
    "# with torch.inference_mode(mode=True):\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    # negative_prompt=negative_prompt,  # no effect\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=0,\n",
    "    # guidance_scale=1,\n",
    "    width=512, height=512, # 0.3s\n",
    "    # width=256, height=256, # 0.17s ~100ms\n",
    "    # clip_skip=3,\n",
    "    generator=g,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b9eed-fd6a-4406-bcb0-002b258d8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc830c1d-fd9e-4be8-8535-943d0d1f1cd2",
   "metadata": {},
   "source": [
    "Try CoreML based stuff:\n",
    "\n",
    "SDXS model already converted:\n",
    "https://huggingface.co/lsb/6-bit-palettized-sdxs-512-dreamshaper/tree/main\n",
    "\n",
    "Look at examples here:\n",
    "https://huggingface.co/apple/coreml-stable-diffusion-2-1-base-palettized\n",
    "\n",
    "Inference like that:\n",
    "https://github.com/apple/ml-stable-diffusion/blob/main/python_coreml_stable_diffusion/pipeline.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3478d595-4e7e-49ad-ac60-67e16758aea5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from diffusers import PixArtAlphaPipeline\n",
    "import torch\n",
    "\n",
    "torch_dtype = torch.float16\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# https://huggingface.co/PixArt-alpha/PixArt-XL-2-256x256\n",
    "# https://huggingface.co/PixArt-alpha/PixArt-XL-2-256x256/blob/main/transformer/diffusion_pytorch_model.safetensors\n",
    "# pipe = PixArtAlphaPipeline.from_pretrained(\"PixArt-alpha/PixArt-XL-2-1024-MS\", torch_dtype=torch.float16)\n",
    "pipe = PixArtAlphaPipeline.from_pretrained(\"raman07/pixart-alpha-256x256\", torch_dtype=torch.float16)\n",
    "\n",
    "pipe.to(device=device, dtype=torch_dtype)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e0eadc2-9021-42b9-a8d4-4aa5e2611d97",
   "metadata": {},
   "source": [
    "prompt = \"award winning photography of Tokyo, Japan, detailed, 8k, daytime, cheerful\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # disable guidance_scale by passing 0\n",
    "    # image = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0).images[0]\n",
    "    image = pipe(\n",
    "        prompt=prompt,\n",
    "        # guidance_scale should be 0, or in 1-2\n",
    "        # num_inference_steps=4, guidance_scale=0,\n",
    "        # num_inference_steps=4, guidance_scale=1,\n",
    "        # num_inference_steps=4, guidance_scale=2,\n",
    "        num_inference_steps=6, guidance_scale=0,  # 4s\n",
    "        width=512, height=512, # 3s\n",
    "        # width=256, height=256, # 0.9s\n",
    "    ).images[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e59a7da8-1f7d-4fd9-a593-9c1fd7caaefe",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import torch\n",
    "\n",
    "from diffusers import DiffusionPipeline, AutoencoderTiny\n",
    "\n",
    "# from diffusers import LCMScheduler, AutoPipelineForText2Image\n",
    "\n",
    "base_model = \"SimianLuo/LCM_Dreamshaper_v7\"\n",
    "taesd_model = \"madebyollin/taesd\"\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e77ac4a6-71c8-417b-8c1d-7c48b99d90cf",
   "metadata": {},
   "source": [
    "torch_dtype = torch.float16\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "pipe.to(device=device, dtype=torch_dtype)\n",
    "\n",
    "results = self.pipe(\n",
    "    prompt_embeds=prompt_embeds,\n",
    "    prompt=prompt,\n",
    "    generator=generator,\n",
    "    num_inference_steps=params.steps,\n",
    "    guidance_scale=params.guidance_scale,\n",
    "    width=params.width,\n",
    "    height=params.height,\n",
    "    output_type=\"pil\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea67ac6-5da3-44d8-a632-3ec41a1a9d47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "2024/10/21\n",
    "\n",
    "TODO: try that https://github.com/ml-explore/mlx-examples/tree/main/flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26c7e3f-ea52-47b0-a9db-0bd3b3181c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef921d9319e543a1b6469c225c05ba37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/536 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b806ebae64c54d5d9e0da0298f8a5305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 23 files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33fa7e474954f14b22fc15b471083d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfdd9189cf64063b27d4bd5c395b71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)t_encoder_2/model.safetensors.index.json:   0%|          | 0.00/19.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8cf67f6a0343d9a4a30c8c87d1d4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_2/config.json:   0%|          | 0.00/782 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14f751dad1b4c1e800fcd84b7a1de10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b56cb0196fd410083a9d48bd3f29657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e14aa3708b456d9bf5ec6758aaa75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eba4385db4a40fb8a9ba7937e46767f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/tokenizer_config.json:   0%|          | 0.00/705 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3ee314a93a4cfc804c2416d65c2c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d79ae657f254370940c4b48da38de3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/special_tokens_map.json:   0%|          | 0.00/588 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0626591c1cd46b2b2995629a9dda867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbcc2ca6d454502b8f6d3c32b09502e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler/scheduler_config.json:   0%|          | 0.00/274 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d849df609f204483be664d023fb5ace8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_2/special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f669ae1ba5040129bc0a65308115df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_2/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f4403e00b442679cf419d16b188100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_2/tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257d4cc9056242db9f0c0507f2d06c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/config.json:   0%|          | 0.00/321 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee7b8913d5342e6b753bf9eeccdb50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405c27129f6c4fdfb90274e4f67daa4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00001-of-00003.safetensors:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed04a8080d54402ae591aaeb22ff0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00002-of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348d3008a3f74927b770259e268b74e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00003-of-00003.safetensors:   0%|          | 0.00/3.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ecfb5a84b148b98495049a88d2ef60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ion_pytorch_model.safetensors.index.json:   0%|          | 0.00/121k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb599b500e26403ca21b2aaced705f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/config.json:   0%|          | 0.00/774 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9f924abc2e40e2beb48b06b3cd86de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/madebyollin/taef1\n",
    "# https://github.com/madebyollin/taesd\n",
    "\n",
    "import torch\n",
    "from diffusers import FluxPipeline, AutoencoderTiny\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-schnell\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipe.vae = AutoencoderTiny.from_pretrained(\n",
    "    \"madebyollin/taef1\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "pipe.enable_sequential_cpu_offload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f44e8-d389-414d-b362-d69a8d905cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"slice of delicious New York-style berry cheesecake\"\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    guidance_scale=0.0,\n",
    "    num_inference_steps=4,\n",
    "    max_sequence_length=256,\n",
    ").images[0]\n",
    "# image.save(\"cheesecake.png\")\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
